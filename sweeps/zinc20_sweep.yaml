method: random
program: train.py
project: "zinc20-electra"
metric:
  goal: minimize
  name: val_loss
run_cap: 500
parameters:
  dataset_name:
    value: "zpn/zinc20"

  tokenizer_name:
    value: "zpn/zinc20_wordlevel_dissociation"

  generator_config:
    value: "model/configs/deberta/generator/tiny.yaml"

  discriminator_config:
    value: "model/configs/deberta/discriminator/tiny.yaml"

  base_config_size:
    value: "tiny.yaml"

  output_mult:
    values: [1, 8, 64, 256, 512, 1024]

  prenorm:
    values: [false, true]

  attn_norm_layer_type:
    values: ["layer_norm", "group_norm"]

  attn_num_groups:
    values: [1, 2, 4, 8]

  embedding_norm_layer_type:
    values: ["layer_norm", "group_norm"]

  embedding_num_groups:
    values: [1, 2, 4, 8]

  mup:
    value: true

  readout_zero_init:
    value: true

  query_zero_init:
    value: true

  lr:
    distribution: uniform
    min: 9e-6
    max: 1e-3

  train_batch_size:
    value: 128

  validation_batch_size:
    value: 256

  num_epochs:
    value: 10

  num_steps_per_epoch:
    value: 1000

  num_eval_steps:
    value: 1000

  num_warmup_steps:
    values: [0, 100, 1000]

  num_training_steps:
    value: 10000

  scheduler:
    values: [false, true]

  global_clip_norm:
    values: [0, 0.2, 0.5, 1]

  arch_name:
    value: deberta

  wandb:
    value: true

  wandb_project:
    value: "zinc20-electra"

  wandb_entity:
    value: "zanussbaum"

  log_predictions:
    value: true


command:
  - accelerate
  - launch
  - ${program}
  - ${args_no_boolean_flags}